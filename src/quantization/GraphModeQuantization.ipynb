{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FaceLandmarkDetection.src.quantization.helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            model(image)\n",
    "            \n",
    "import os\n",
    "def print_size_of_model(model):\n",
    "    if isinstance(model, torch.jit.RecursiveScriptModule):\n",
    "        torch.jit.save(model, \"temp.p\")\n",
    "    else:\n",
    "        torch.jit.save(torch.jit.script(model), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 136\n",
    "cuda_device = torch.device(\"cuda:0\")\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "model_dir = \"checkpoints\"\n",
    "model_filename = \"resnet18_FLM.pt\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1111\n"
     ]
    }
   ],
   "source": [
    "# Create an untrained model.\n",
    "model = create_model(num_classes=136)\n",
    "# Load a pretrained model.\n",
    "float_model = load_model(model=model, model_filepath=model_filepath, device=cpu_device)\n",
    "\n",
    "train_dataset, val_dataset = get_data()\n",
    "train_loader = make_loader(train_dataset, 64)\n",
    "val_loader = make_loader(val_dataset, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model = torch.jit.script(float_model.eval()) # or torch.jit.trace(float_model, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATIC QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "QConfig(activation=functools.partial(<class 'torch.quantization.observer.HistogramObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import torch\n",
    "from torch.quantization import get_default_qconfig, quantize_jit\n",
    "qconfig = get_default_qconfig('fbgemm')\n",
    "qconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = quantize_jit(\n",
    "    ts_model, # TorchScript model\n",
    "    {'': qconfig}, # qconfig dict\n",
    "    calibrate, # calibration function\n",
    "    [val_loader]) # positional arguments to calibration function, typically some sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of model before quantization\nSize (MB): 45.075501\nSize of model after quantization\nSize (MB): 11.355301\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of model before quantization\")\n",
    "print_size_of_model(ts_model)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.04602653190896318"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "evaluate_model(model=model, test_loader=val_loader, device=cpu_device, criterion=nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FP32 CPU Inference Latency: 1613.90 ms / sample\nINT8 CPU Inference Latency: 274.68 ms /sample\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency = measure_inference_latency(model=ts_model, device=cpu_device, input_size=(32,1,224,224), num_samples=100)\n",
    "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(32,1,224,224), num_samples=100)\n",
    "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
    "print(\"INT8 CPU Inference Latency: {:.2f} ms /sample\".format(int8_cpu_inference_latency * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DYNAMIC QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.quantization import per_channel_dynamic_qconfig\n",
    "from torch.quantization import quantize_dynamic_jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'': QConfigDynamic(activation=<class 'torch.quantization.observer.MinMaxDynamicQuantObserver'>, weight=functools.partial(<class 'torch.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "qconfig_dict = {'': per_channel_dynamic_qconfig}\n",
    "qconfig_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = quantize_dynamic_jit(ts_model, qconfig_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of model before quantization\nSize (MB): 45.075181\nSize of model after quantization\nSize (MB): 44.750093\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of model before quantization\")\n",
    "print_size_of_model(ts_model)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INT8 evaluation loss: 0.048\n"
     ]
    }
   ],
   "source": [
    "int8_eval_loss = evaluate_model(model=quantized_model, test_loader=val_loader, device=cpu_device, criterion=nn.MSELoss())\n",
    "# Skip this assertion since the values might deviate a lot.\n",
    "# assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
    "\n",
    "print(\"INT8 evaluation loss: {:.3f}\".format(int8_eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(32,1,224,224), num_samples=100)\n",
    "print(\"INT8 CPU Inference Latency: {:.2f} ms /sample\".format(int8_cpu_inference_latency * 1000))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NO SUPPORT FOR QAT IN GRAPH MODE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INT8 CPU Inference Latency: 1666.39 ms /sample\n"
     ]
    }
   ],
   "source": []
  },
  {
   "source": [
    "NO SUPPORT FOR QAT IN GRAPH MODE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}