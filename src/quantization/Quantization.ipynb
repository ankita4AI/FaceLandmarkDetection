{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POST TRAINING STATIC QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from FaceLandmarkDetection.src.quantization.helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "num_classes = 136\n",
    "cuda_device = torch.device(\"cuda:0\")\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "model_dir = \"checkpoints\"\n",
    "model_filename = \"resnet18_FLM.pt\"\n",
    "quantized_model_dir = \"checkpoints_quantized\"\n",
    "quantized_model_filename = \"resnet18_FLM_quantized.pt\"\n",
    "qat_quantized_model_filename = \"resnet18_FLM_quantized_qat.pt\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "quantized_model_filepath = os.path.join(quantized_model_dir, quantized_model_filename)\n",
    "qat_quantized_model_filepath = os.path.join(quantized_model_dir, qat_quantized_model_filename)\n",
    "set_random_seeds(random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1111\n"
     ]
    }
   ],
   "source": [
    "# Create an untrained model.\n",
    "model = create_model(num_classes=num_classes)\n",
    "train_dataset, val_dataset = get_data()\n",
    "train_loader = make_loader(train_dataset, 64)\n",
    "val_loader = make_loader(val_dataset, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model=model, model_filepath=model_filepath, device=cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the model for layer fusion\n",
    "fused_model = copy.deepcopy(model)\n",
    "\n",
    "model.eval()\n",
    "# The model has to be switched to evaluation mode before any layer fusion.\n",
    "# Otherwise the quantization will not work correctly.\n",
    "fused_model.eval()\n",
    "\n",
    "# Fuse the model in place rather manually.\n",
    "fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
    "for module_name, module in fused_model.named_children():\n",
    "    if \"layer\" in module_name:\n",
    "        for basic_block_name, basic_block in module.named_children():\n",
    "            torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
    "            for sub_block_name, sub_block in basic_block.named_children():\n",
    "                if sub_block_name == \"downsample\":\n",
    "                    torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print FP32 model.\n",
    "print(model)\n",
    "# Print fused model.\n",
    "print(fused_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ResNet(\n  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=136, bias=True)\n)\nResNet(\n  (conv1): ConvReLU2d(\n    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n    (1): ReLU(inplace=True)\n  )\n  (bn1): Identity()\n  (relu): Identity()\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): ConvReLU2d(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU(inplace=True)\n      )\n      (bn1): Identity()\n      (relu1): Identity()\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n    (1): BasicBlock(\n      (conv1): ConvReLU2d(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU(inplace=True)\n      )\n      (bn1): Identity()\n      (relu1): Identity()\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): ConvReLU2d(\n        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): ReLU(inplace=True)\n      )\n      (bn1): Identity()\n      (relu1): Identity()\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n    (1): BasicBlock(\n      (conv1): ConvReLU2d(\n        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU(inplace=True)\n      )\n      (bn1): Identity()\n      (relu1): Identity()\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): ConvReLU2d(\n        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): ReLU(inplace=True)\n      )\n      (bn1): Identity()\n      (relu1): Identity()\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n    (1): BasicBlock(\n      (conv1): ConvReLU2d(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU(inplace=True)\n      )\n      (bn1): Identity()\n      (relu1): Identity()\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): ConvReLU2d(\n        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): ReLU(inplace=True)\n      )\n      (bn1): Identity()\n      (relu1): Identity()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n    (1): BasicBlock(\n      (conv1): ConvReLU2d(\n        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU(inplace=True)\n      )\n      (bn1): Identity()\n      (relu1): Identity()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n      (relu2): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=136, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
    "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "quantized_model.qconfig = quantization_config\n",
    "\n",
    "print(quantized_model.qconfig)\n",
    "\n",
    "torch.quantization.prepare(quantized_model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.HistogramObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "QuantizedResNet18(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (model_fp32): ResNet(\n",
       "    (conv1): ConvReLU2d(\n",
       "      (0): Conv2d(\n",
       "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): ReLU(\n",
       "        inplace=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "    (bn1): Identity()\n",
       "    (relu): Identity()\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): ConvReLU2d(\n",
       "          (0): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): ReLU(\n",
       "            inplace=True\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(\n",
       "          inplace=True\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): ConvReLU2d(\n",
       "          (0): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): ReLU(\n",
       "            inplace=True\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(\n",
       "          inplace=True\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): ConvReLU2d(\n",
       "          (0): Conv2d(\n",
       "            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): ReLU(\n",
       "            inplace=True\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(\n",
       "            64, 128, kernel_size=(1, 1), stride=(2, 2)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(\n",
       "          inplace=True\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): ConvReLU2d(\n",
       "          (0): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): ReLU(\n",
       "            inplace=True\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(\n",
       "          inplace=True\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): ConvReLU2d(\n",
       "          (0): Conv2d(\n",
       "            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): ReLU(\n",
       "            inplace=True\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(\n",
       "            128, 256, kernel_size=(1, 1), stride=(2, 2)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(\n",
       "          inplace=True\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): ConvReLU2d(\n",
       "          (0): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): ReLU(\n",
       "            inplace=True\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(\n",
       "          inplace=True\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): ConvReLU2d(\n",
       "          (0): Conv2d(\n",
       "            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): ReLU(\n",
       "            inplace=True\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(\n",
       "          inplace=True\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): ConvReLU2d(\n",
       "          (0): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): ReLU(\n",
       "            inplace=True\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "        )\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (bn2): Identity()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu2): ReLU(\n",
       "          inplace=True\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(\n",
       "      in_features=512, out_features=136, bias=True\n",
       "      (activation_post_process): HistogramObserver()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
    "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "quantized_model.qconfig = quantization_config\n",
    "\n",
    "print(quantized_model.qconfig)\n",
    "\n",
    "torch.quantization.prepare(quantized_model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use training data for calibration.\n",
    "calibrate_model(model=quantized_model, loader=val_loader, device=cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "QuantizedResNet18(\n  (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n  (dequant): DeQuantize()\n  (model_fp32): ResNet(\n    (conv1): QuantizedConvReLU2d(1, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.02210974507033825, zero_point=0, padding=(3, 3))\n    (bn1): Identity()\n    (relu): Identity()\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.016083866357803345, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.054224684834480286, zero_point=64, padding=(1, 1))\n        (bn2): Identity()\n        (skip_add): QFunctional(\n          scale=0.05315326526761055, zero_point=60\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.014571553096175194, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.053782589733600616, zero_point=71, padding=(1, 1))\n        (bn2): Identity()\n        (skip_add): QFunctional(\n          scale=0.06847162544727325, zero_point=54\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.017052551731467247, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.04284200444817543, zero_point=65, padding=(1, 1))\n        (bn2): Identity()\n        (downsample): Sequential(\n          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.031649816781282425, zero_point=69)\n          (1): Identity()\n        )\n        (skip_add): QFunctional(\n          scale=0.048472363501787186, zero_point=67\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.013343805447220802, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.04280572384595871, zero_point=75, padding=(1, 1))\n        (bn2): Identity()\n        (skip_add): QFunctional(\n          scale=0.06172383949160576, zero_point=55\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.01522653829306364, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.04708382487297058, zero_point=52, padding=(1, 1))\n        (bn2): Identity()\n        (downsample): Sequential(\n          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.01422104425728321, zero_point=88)\n          (1): Identity()\n        )\n        (skip_add): QFunctional(\n          scale=0.04721663147211075, zero_point=62\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.012054764665663242, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.041550517082214355, zero_point=78, padding=(1, 1))\n        (bn2): Identity()\n        (skip_add): QFunctional(\n          scale=0.0432019904255867, zero_point=70\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.011365827172994614, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.045488543808460236, zero_point=70, padding=(1, 1))\n        (bn2): Identity()\n        (downsample): Sequential(\n          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.037580594420433044, zero_point=58)\n          (1): Identity()\n        )\n        (skip_add): QFunctional(\n          scale=0.04605971276760101, zero_point=67\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.010693843476474285, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.19435882568359375, zero_point=48, padding=(1, 1))\n        (bn2): Identity()\n        (skip_add): QFunctional(\n          scale=0.21134546399116516, zero_point=44\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): QuantizedLinear(in_features=512, out_features=136, scale=0.01499930489808321, zero_point=61, qscheme=torch.per_channel_affine)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
    "quantized_model.eval()\n",
    "# Print quantized model.\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save quantized model.\n",
    "save_torchscript_model(model=quantized_model, model_dir=quantized_model_dir, model_filename=quantized_model_filename)\n",
    "# Load quantized model.\n",
    "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_eval_loss = evaluate_model(model=model, test_loader=val_loader, device=cpu_device, criterion=nn.MSELoss())\n",
    "int8_eval_loss = evaluate_model(model=quantized_jit_model, test_loader=val_loader, device=cpu_device, criterion=nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FP32 evaluation loss: 0.023\nINT8 evaluation loss: 0.025\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 evaluation loss: {:.3f}\".format(fp32_eval_loss))\n",
    "print(\"INT8 evaluation loss: {:.3f}\".format(int8_eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(32,1,224,224), num_samples=100)\n",
    "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(32,1,224,224), num_samples=100)\n",
    "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(32,1,224,224), num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FP32 CPU Inference Latency: 1628.89 ms / sample\nINT8 CPU Inference Latency: 280.72 ms / sample\nINT8 JIT CPU Inference Latency: 316.39 ms / sample\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
    "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
    "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size (MB): 11.371467\n",
      "Size (MB): 45.076717\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(quantized_jit_model)\n",
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POST TRAINING DYNAMIC QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a quantized model instance\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model,  # the original model\n",
    "    dtype=torch.qint8)  # the target dtype for quantized weights\n",
    "\n",
    "# run the model\n",
    "input_fp32 = torch.randn(32, 1, 224, 224)\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of model before quantization\n",
      "Size (MB): 45.076717\n",
      "Size of model after quantization\n",
      "Size (MB): 44.871211\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of model before quantization\")\n",
    "print_size_of_model(model)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(model_int8)"
   ]
  },
  {
   "source": [
    "Dynamic quantization only helps in reducing the model size for models that use Linear and LSTM modules. For the case of resnet18, the model consists of conv layers which do not have dynamic quantization support yet.# Dynamic quantization only helps in reducing the model size for models that use Linear and LSTM modules. For the case of resnet18, the model consists of conv layers which do not have dynamic quantization support yet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "QUANTIZATION AWARE TRAINING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ResNet(\n",
      "  (conv1): ConvBnReLU2d(\n",
      "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (bn1): Identity()\n",
      "  (relu): Identity()\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): ConvBn2d(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): ConvBn2d(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): ConvBn2d(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=136, bias=True)\n",
      ")\n",
      "ResNet(\n",
      "  (conv1): ConvBnReLU2d(\n",
      "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (bn1): Identity()\n",
      "  (relu): Identity()\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): ConvBn2d(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): ConvBn2d(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): ConvBn2d(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): ConvBnReLU2d(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): ConvBn2d(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=136, bias=True)\n",
      ")\n",
      "Epoch: 00 Train Loss: 8.151 Eval Loss: 26.968\n",
      "Epoch: 01 Train Loss: 45.411 Eval Loss: 63.264\n",
      "Epoch: 02 Train Loss: 90.090 Eval Loss: 109.720\n",
      "Epoch: 03 Train Loss: 116.853 Eval Loss: 49.015\n",
      "Epoch: 04 Train Loss: 23.947 Eval Loss: 9.441\n",
      "Epoch: 05 Train Loss: 3.610 Eval Loss: 1.748\n",
      "Epoch: 06 Train Loss: 1.087 Eval Loss: 0.879\n",
      "Epoch: 07 Train Loss: 0.706 Eval Loss: 0.717\n",
      "Epoch: 08 Train Loss: 0.584 Eval Loss: 0.597\n",
      "Epoch: 09 Train Loss: 0.539 Eval Loss: 0.564\n"
     ]
    }
   ],
   "source": [
    "model = create_model(num_classes=136)\n",
    "model = load_model(model=model, model_filepath=model_filepath, device=cpu_device)\n",
    "model.to(cpu_device)\n",
    "# Make a copy of the model for layer fusion\n",
    "fused_model = copy.deepcopy(model)\n",
    "\n",
    "model.train()\n",
    "# The model has to be switched to training mode before any layer fusion.\n",
    "# Otherwise the quantization aware training will not work correctly.\n",
    "fused_model.train()\n",
    "\n",
    "fused_model = torch.quantization.fuse_modules(model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
    "for module_name, module in fused_model.named_children():\n",
    "    if \"layer\" in module_name:\n",
    "        for basic_block_name, basic_block in module.named_children():\n",
    "            torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
    "            for sub_block_name, sub_block in basic_block.named_children():\n",
    "                if sub_block_name == \"downsample\":\n",
    "                    torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
    "\n",
    "\n",
    "print(model)\n",
    "print(fused_model)\n",
    "\n",
    "model.eval()\n",
    "fused_model.eval()                  \n",
    "\n",
    "fused_model_w_quant = QuantizedResNet18(model_fp32=fused_model)\n",
    "fused_model_w_quant.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "model_fp32_prepared = torch.quantization.prepare_qat(fused_model_w_quant)\n",
    "\n",
    "# Train model.\n",
    "model_fp32_prepared = train_model(model=model_fp32_prepared, train_loader=train_loader, test_loader=val_loader, device=cpu_device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "QuantizedResNet18(\n",
       "  (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       "  (model_fp32): ResNet(\n",
       "    (conv1): QuantizedConvReLU2d(1, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.03440048545598984, zero_point=0, padding=(3, 3))\n",
       "    (bn1): Identity()\n",
       "    (relu): Identity()\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.028485286980867386, zero_point=0, padding=(1, 1))\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.07750637829303741, zero_point=76, padding=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.08231262117624283, zero_point=62\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (relu2): QuantizedReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.025983158499002457, zero_point=0, padding=(1, 1))\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.08603127300739288, zero_point=71, padding=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.10299002379179001, zero_point=56\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (relu2): QuantizedReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.025228437036275864, zero_point=0, padding=(1, 1))\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.0623946487903595, zero_point=65, padding=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.05323358625173569, zero_point=61)\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.0715649351477623, zero_point=67\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (relu2): QuantizedReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.02001144364476204, zero_point=0, padding=(1, 1))\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.06960275769233704, zero_point=70, padding=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.08579635620117188, zero_point=54\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (relu2): QuantizedReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.023233890533447266, zero_point=0, padding=(1, 1))\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.06874243915081024, zero_point=48, padding=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.022829005494713783, zero_point=82)\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.07137593626976013, zero_point=53\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (relu2): QuantizedReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.021430635824799538, zero_point=0, padding=(1, 1))\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.0799156203866005, zero_point=81, padding=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.0668722465634346, zero_point=62\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (relu2): QuantizedReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.01983044296503067, zero_point=0, padding=(1, 1))\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.07411748170852661, zero_point=76, padding=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (downsample): Sequential(\n",
       "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.0516001358628273, zero_point=60)\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.06538001447916031, zero_point=65\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (relu2): QuantizedReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.016473447903990746, zero_point=0, padding=(1, 1))\n",
       "        (bn1): Identity()\n",
       "        (relu1): Identity()\n",
       "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.25072386860847473, zero_point=47, padding=(1, 1))\n",
       "        (bn2): Identity()\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.2541186511516571, zero_point=45\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (relu2): QuantizedReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): QuantizedLinear(in_features=512, out_features=136, scale=0.11874838918447495, zero_point=80, qscheme=torch.per_channel_affine)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "quantized_model = torch.quantization.convert(model_fp32_prepared.eval(), inplace=False)\n",
    "quantized_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "QuantizedResNet18(\n  (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n  (dequant): DeQuantize()\n  (model_fp32): ResNet(\n    (conv1): QuantizedConvReLU2d(1, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.03440048545598984, zero_point=0, padding=(3, 3))\n    (bn1): Identity()\n    (relu): Identity()\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.028485286980867386, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.07750637829303741, zero_point=76, padding=(1, 1))\n        (bn2): Identity()\n        (skip_add): QFunctional(\n          scale=0.08231262117624283, zero_point=62\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.025983158499002457, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.08603127300739288, zero_point=71, padding=(1, 1))\n        (bn2): Identity()\n        (skip_add): QFunctional(\n          scale=0.10299002379179001, zero_point=56\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.025228437036275864, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.0623946487903595, zero_point=65, padding=(1, 1))\n        (bn2): Identity()\n        (downsample): Sequential(\n          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.05323358625173569, zero_point=61)\n          (1): Identity()\n        )\n        (skip_add): QFunctional(\n          scale=0.0715649351477623, zero_point=67\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.02001144364476204, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.06960275769233704, zero_point=70, padding=(1, 1))\n        (bn2): Identity()\n        (skip_add): QFunctional(\n          scale=0.08579635620117188, zero_point=54\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.023233890533447266, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.06874243915081024, zero_point=48, padding=(1, 1))\n        (bn2): Identity()\n        (downsample): Sequential(\n          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.022829005494713783, zero_point=82)\n          (1): Identity()\n        )\n        (skip_add): QFunctional(\n          scale=0.07137593626976013, zero_point=53\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.021430635824799538, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.0799156203866005, zero_point=81, padding=(1, 1))\n        (bn2): Identity()\n        (skip_add): QFunctional(\n          scale=0.0668722465634346, zero_point=62\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.01983044296503067, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.07411748170852661, zero_point=76, padding=(1, 1))\n        (bn2): Identity()\n        (downsample): Sequential(\n          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.0516001358628273, zero_point=60)\n          (1): Identity()\n        )\n        (skip_add): QFunctional(\n          scale=0.06538001447916031, zero_point=65\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.016473447903990746, zero_point=0, padding=(1, 1))\n        (bn1): Identity()\n        (relu1): Identity()\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.25072386860847473, zero_point=47, padding=(1, 1))\n        (bn2): Identity()\n        (skip_add): QFunctional(\n          scale=0.2541186511516571, zero_point=45\n          (activation_post_process): Identity()\n        )\n        (relu2): QuantizedReLU(inplace=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): QuantizedLinear(in_features=512, out_features=136, scale=0.11874838918447495, zero_point=80, qscheme=torch.per_channel_affine)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save quantized model.\n",
    "save_torchscript_model(model=quantized_model, model_dir=quantized_model_dir, model_filename=qat_quantized_model_filename)\n",
    "# Load quantized model.\n",
    "quantized_jit_model = load_torchscript_model(model_filepath=qat_quantized_model_filepath, device=cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INT8 evaluation accuracy: 0.505\n"
     ]
    }
   ],
   "source": [
    "int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=val_loader, device=cpu_device, criterion=nn.MSELoss())\n",
    "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FP32 CPU Inference Latency: 1625.51 ms / sample\nINT8 CPU Inference Latency: 306.87 ms / sample\nINT8 JIT CPU Inference Latency: 334.12 ms / sample\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(32,1,224,224), num_samples=100)\n",
    "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(32,1,224,224), num_samples=100)\n",
    "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(32,1,224,224), num_samples=100)\n",
    "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
    "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
    "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}