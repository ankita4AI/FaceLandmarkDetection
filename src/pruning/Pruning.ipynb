{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "from FaceLandmarkDetection.src.quantization.helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "num_classes = 136\n",
    "cuda_device = torch.device(\"cuda:0\")\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "model_dir = \"checkpoints\"\n",
    "model_filename = \"resnet18_FLM.pt\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "set_random_seeds(random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1111\n"
     ]
    }
   ],
   "source": [
    "# Create an untrained model.\n",
    "model = create_model(num_classes=num_classes)\n",
    "train_dataset, val_dataset = get_data()\n",
    "train_loader = make_loader(train_dataset, 64)\n",
    "val_loader = make_loader(val_dataset, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model=model, model_filepath=model_filepath, device=cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('weight', Parameter containing:\ntensor([[[[ 0.0967,  0.0468,  0.1814,  ..., -0.1097, -0.0182, -0.0594],\n          [-0.0778,  0.0543,  0.1558,  ..., -0.0382, -0.1938,  0.0518],\n          [ 0.1366,  0.0528,  0.0694,  ..., -0.0286, -0.0990, -0.0428],\n          ...,\n          [ 0.0738,  0.1917,  0.0005,  ..., -0.2320, -0.0540, -0.0515],\n          [ 0.1891,  0.0243,  0.0659,  ..., -0.0716,  0.0362,  0.0614],\n          [ 0.1064,  0.1939,  0.2091,  ..., -0.0244, -0.0978, -0.1774]]],\n\n\n        [[[ 0.1092,  0.0919,  0.0567,  ..., -0.0969,  0.0725,  0.1699],\n          [-0.1208,  0.0059,  0.0228,  ..., -0.1751, -0.0929, -0.0746],\n          [ 0.0866, -0.0080, -0.0398,  ..., -0.1616, -0.0858, -0.0612],\n          ...,\n          [ 0.1268,  0.1058,  0.1897,  ...,  0.0847, -0.0484,  0.0984],\n          [ 0.1092,  0.0135,  0.0963,  ...,  0.1790,  0.1162,  0.0350],\n          [ 0.0162,  0.1458,  0.1452,  ...,  0.1125,  0.0016,  0.0655]]],\n\n\n        [[[ 0.0383, -0.0722, -0.0494,  ...,  0.0340, -0.1325,  0.0605],\n          [-0.1214, -0.1141,  0.0681,  ...,  0.0020,  0.0466,  0.0538],\n          [-0.0033, -0.0364,  0.0399,  ...,  0.0254,  0.0523, -0.0556],\n          ...,\n          [ 0.0195,  0.0642, -0.0454,  ...,  0.1366, -0.1053,  0.0735],\n          [ 0.0364, -0.1416,  0.0343,  ..., -0.0144, -0.0516,  0.0924],\n          [-0.0867,  0.1023,  0.0913,  ...,  0.1291, -0.0760, -0.0866]]],\n\n\n        ...,\n\n\n        [[[-0.1936, -0.1919, -0.0589,  ..., -0.0512, -0.0491, -0.1262],\n          [-0.1462,  0.0370, -0.0874,  ..., -0.0858, -0.0785, -0.1244],\n          [-0.0265,  0.0593, -0.0094,  ..., -0.1230,  0.0176, -0.0836],\n          ...,\n          [-0.0491, -0.0144, -0.0266,  ...,  0.0300, -0.0560,  0.0528],\n          [-0.0822,  0.0284, -0.1209,  ..., -0.1258, -0.1998, -0.0681],\n          [-0.0559,  0.0116, -0.1333,  ...,  0.0403, -0.0493, -0.2034]]],\n\n\n        [[[ 0.0774,  0.0112,  0.0672,  ...,  0.2061,  0.1921,  0.2375],\n          [ 0.0789, -0.1160,  0.1890,  ...,  0.0762, -0.0308,  0.1433],\n          [ 0.0701,  0.0425,  0.1911,  ...,  0.1647,  0.0801,  0.1504],\n          ...,\n          [ 0.0417, -0.0374, -0.0327,  ...,  0.0086, -0.0826,  0.1660],\n          [-0.0260,  0.1226, -0.0502,  ...,  0.0134,  0.1247,  0.0881],\n          [-0.0442,  0.0257,  0.1092,  ...,  0.0074,  0.0324,  0.1421]]],\n\n\n        [[[-0.0652, -0.0241, -0.1276,  ...,  0.0156, -0.0609,  0.0838],\n          [-0.0108, -0.1463, -0.0090,  ..., -0.0941, -0.0158, -0.2100],\n          [-0.1405, -0.1867, -0.1374,  ..., -0.0933, -0.2279, -0.0453],\n          ...,\n          [ 0.0750,  0.0070, -0.0668,  ..., -0.1734, -0.0856, -0.1139],\n          [-0.1466,  0.0111,  0.0306,  ...,  0.0878, -0.1145,  0.1248],\n          [-0.0317, -0.0272, -0.1724,  ..., -0.0065,  0.1203, -0.0422]]]],\n       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "module = model.conv1\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('weight_orig', Parameter containing:\ntensor([[[[ 0.0967,  0.0468,  0.1814,  ..., -0.1097, -0.0182, -0.0594],\n          [-0.0778,  0.0543,  0.1558,  ..., -0.0382, -0.1938,  0.0518],\n          [ 0.1366,  0.0528,  0.0694,  ..., -0.0286, -0.0990, -0.0428],\n          ...,\n          [ 0.0738,  0.1917,  0.0005,  ..., -0.2320, -0.0540, -0.0515],\n          [ 0.1891,  0.0243,  0.0659,  ..., -0.0716,  0.0362,  0.0614],\n          [ 0.1064,  0.1939,  0.2091,  ..., -0.0244, -0.0978, -0.1774]]],\n\n\n        [[[ 0.1092,  0.0919,  0.0567,  ..., -0.0969,  0.0725,  0.1699],\n          [-0.1208,  0.0059,  0.0228,  ..., -0.1751, -0.0929, -0.0746],\n          [ 0.0866, -0.0080, -0.0398,  ..., -0.1616, -0.0858, -0.0612],\n          ...,\n          [ 0.1268,  0.1058,  0.1897,  ...,  0.0847, -0.0484,  0.0984],\n          [ 0.1092,  0.0135,  0.0963,  ...,  0.1790,  0.1162,  0.0350],\n          [ 0.0162,  0.1458,  0.1452,  ...,  0.1125,  0.0016,  0.0655]]],\n\n\n        [[[ 0.0383, -0.0722, -0.0494,  ...,  0.0340, -0.1325,  0.0605],\n          [-0.1214, -0.1141,  0.0681,  ...,  0.0020,  0.0466,  0.0538],\n          [-0.0033, -0.0364,  0.0399,  ...,  0.0254,  0.0523, -0.0556],\n          ...,\n          [ 0.0195,  0.0642, -0.0454,  ...,  0.1366, -0.1053,  0.0735],\n          [ 0.0364, -0.1416,  0.0343,  ..., -0.0144, -0.0516,  0.0924],\n          [-0.0867,  0.1023,  0.0913,  ...,  0.1291, -0.0760, -0.0866]]],\n\n\n        ...,\n\n\n        [[[-0.1936, -0.1919, -0.0589,  ..., -0.0512, -0.0491, -0.1262],\n          [-0.1462,  0.0370, -0.0874,  ..., -0.0858, -0.0785, -0.1244],\n          [-0.0265,  0.0593, -0.0094,  ..., -0.1230,  0.0176, -0.0836],\n          ...,\n          [-0.0491, -0.0144, -0.0266,  ...,  0.0300, -0.0560,  0.0528],\n          [-0.0822,  0.0284, -0.1209,  ..., -0.1258, -0.1998, -0.0681],\n          [-0.0559,  0.0116, -0.1333,  ...,  0.0403, -0.0493, -0.2034]]],\n\n\n        [[[ 0.0774,  0.0112,  0.0672,  ...,  0.2061,  0.1921,  0.2375],\n          [ 0.0789, -0.1160,  0.1890,  ...,  0.0762, -0.0308,  0.1433],\n          [ 0.0701,  0.0425,  0.1911,  ...,  0.1647,  0.0801,  0.1504],\n          ...,\n          [ 0.0417, -0.0374, -0.0327,  ...,  0.0086, -0.0826,  0.1660],\n          [-0.0260,  0.1226, -0.0502,  ...,  0.0134,  0.1247,  0.0881],\n          [-0.0442,  0.0257,  0.1092,  ...,  0.0074,  0.0324,  0.1421]]],\n\n\n        [[[-0.0652, -0.0241, -0.1276,  ...,  0.0156, -0.0609,  0.0838],\n          [-0.0108, -0.1463, -0.0090,  ..., -0.0941, -0.0158, -0.2100],\n          [-0.1405, -0.1867, -0.1374,  ..., -0.0933, -0.2279, -0.0453],\n          ...,\n          [ 0.0750,  0.0070, -0.0668,  ..., -0.1734, -0.0856, -0.1139],\n          [-0.1466,  0.0111,  0.0306,  ...,  0.0878, -0.1145,  0.1248],\n          [-0.0317, -0.0272, -0.1724,  ..., -0.0065,  0.1203, -0.0422]]]],\n       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('weight_mask', tensor([[[[0., 1., 1.,  ..., 0., 1., 0.],\n          [1., 1., 1.,  ..., 0., 1., 0.],\n          [0., 1., 1.,  ..., 1., 0., 1.],\n          ...,\n          [1., 1., 1.,  ..., 0., 0., 1.],\n          [1., 1., 0.,  ..., 1., 0., 1.],\n          [1., 1., 0.,  ..., 1., 1., 0.]]],\n\n\n        [[[1., 0., 1.,  ..., 1., 0., 1.],\n          [1., 1., 1.,  ..., 1., 1., 0.],\n          [0., 0., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 0., 0.,  ..., 0., 1., 1.],\n          [0., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 0., 1., 1.]]],\n\n\n        [[[0., 0., 0.,  ..., 1., 0., 1.],\n          [1., 0., 1.,  ..., 1., 0., 1.],\n          [0., 1., 1.,  ..., 0., 0., 1.],\n          ...,\n          [1., 1., 1.,  ..., 0., 1., 1.],\n          [1., 0., 1.,  ..., 0., 1., 1.],\n          [0., 0., 0.,  ..., 1., 0., 1.]]],\n\n\n        ...,\n\n\n        [[[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 0.,  ..., 1., 0., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [0., 1., 1.,  ..., 1., 1., 1.],\n          [0., 1., 1.,  ..., 1., 1., 0.],\n          [1., 1., 1.,  ..., 1., 0., 1.]]],\n\n\n        [[[1., 1., 0.,  ..., 1., 0., 1.],\n          [1., 0., 1.,  ..., 1., 1., 1.],\n          [1., 0., 1.,  ..., 1., 1., 1.],\n          ...,\n          [0., 1., 0.,  ..., 1., 0., 1.],\n          [0., 1., 1.,  ..., 0., 0., 1.],\n          [1., 1., 0.,  ..., 1., 1., 0.]]],\n\n\n        [[[0., 1., 1.,  ..., 1., 1., 1.],\n          [0., 0., 1.,  ..., 0., 1., 0.],\n          [1., 1., 1.,  ..., 1., 0., 1.],\n          ...,\n          [1., 0., 1.,  ..., 1., 0., 1.],\n          [1., 1., 0.,  ..., 1., 1., 0.],\n          [1., 1., 0.,  ..., 0., 1., 1.]]]]))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[ 0.0000,  0.0468,  0.1814,  ..., -0.0000, -0.0182, -0.0000],\n          [-0.0778,  0.0543,  0.1558,  ..., -0.0000, -0.1938,  0.0000],\n          [ 0.0000,  0.0528,  0.0694,  ..., -0.0286, -0.0000, -0.0428],\n          ...,\n          [ 0.0738,  0.1917,  0.0005,  ..., -0.0000, -0.0000, -0.0515],\n          [ 0.1891,  0.0243,  0.0000,  ..., -0.0716,  0.0000,  0.0614],\n          [ 0.1064,  0.1939,  0.0000,  ..., -0.0244, -0.0978, -0.0000]]],\n\n\n        [[[ 0.1092,  0.0000,  0.0567,  ..., -0.0969,  0.0000,  0.1699],\n          [-0.1208,  0.0059,  0.0228,  ..., -0.1751, -0.0929, -0.0000],\n          [ 0.0000, -0.0000, -0.0398,  ..., -0.1616, -0.0858, -0.0612],\n          ...,\n          [ 0.1268,  0.0000,  0.0000,  ...,  0.0000, -0.0484,  0.0984],\n          [ 0.0000,  0.0135,  0.0963,  ...,  0.1790,  0.1162,  0.0350],\n          [ 0.0162,  0.1458,  0.1452,  ...,  0.0000,  0.0016,  0.0655]]],\n\n\n        [[[ 0.0000, -0.0000, -0.0000,  ...,  0.0340, -0.0000,  0.0605],\n          [-0.1214, -0.0000,  0.0681,  ...,  0.0020,  0.0000,  0.0538],\n          [-0.0000, -0.0364,  0.0399,  ...,  0.0000,  0.0000, -0.0556],\n          ...,\n          [ 0.0195,  0.0642, -0.0454,  ...,  0.0000, -0.1053,  0.0735],\n          [ 0.0364, -0.0000,  0.0343,  ..., -0.0000, -0.0516,  0.0924],\n          [-0.0000,  0.0000,  0.0000,  ...,  0.1291, -0.0000, -0.0866]]],\n\n\n        ...,\n\n\n        [[[-0.1936, -0.1919, -0.0589,  ..., -0.0512, -0.0491, -0.1262],\n          [-0.1462,  0.0370, -0.0000,  ..., -0.0858, -0.0000, -0.1244],\n          [-0.0265,  0.0593, -0.0094,  ..., -0.1230,  0.0176, -0.0836],\n          ...,\n          [-0.0000, -0.0144, -0.0266,  ...,  0.0300, -0.0560,  0.0528],\n          [-0.0000,  0.0284, -0.1209,  ..., -0.1258, -0.1998, -0.0000],\n          [-0.0559,  0.0116, -0.1333,  ...,  0.0403, -0.0000, -0.2034]]],\n\n\n        [[[ 0.0774,  0.0112,  0.0000,  ...,  0.2061,  0.0000,  0.2375],\n          [ 0.0789, -0.0000,  0.1890,  ...,  0.0762, -0.0308,  0.1433],\n          [ 0.0701,  0.0000,  0.1911,  ...,  0.1647,  0.0801,  0.1504],\n          ...,\n          [ 0.0000, -0.0374, -0.0000,  ...,  0.0086, -0.0000,  0.1660],\n          [-0.0000,  0.1226, -0.0502,  ...,  0.0000,  0.0000,  0.0881],\n          [-0.0442,  0.0257,  0.0000,  ...,  0.0074,  0.0324,  0.0000]]],\n\n\n        [[[-0.0000, -0.0241, -0.1276,  ...,  0.0156, -0.0609,  0.0838],\n          [-0.0000, -0.0000, -0.0090,  ..., -0.0000, -0.0158, -0.0000],\n          [-0.1405, -0.1867, -0.1374,  ..., -0.0933, -0.0000, -0.0453],\n          ...,\n          [ 0.0750,  0.0000, -0.0668,  ..., -0.1734, -0.0000, -0.1139],\n          [-0.1466,  0.0111,  0.0000,  ...,  0.0878, -0.1145,  0.0000],\n          [-0.0317, -0.0272, -0.0000,  ..., -0.0000,  0.1203, -0.0422]]]],\n       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(module.weight)"
   ]
  },
  {
   "source": [
    "Finally, pruning is applied prior to each forward pass using PyTorch’s forward_pre_hooks. Specifically, when the module is pruned, as we have done here, it will acquire a forward_pre_hook for each parameter associated with it that gets pruned. In this case, since we have so far only pruned the original parameter named weight, only one hook will be present."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x7fae529acd60>)])\n"
     ]
    }
   ],
   "source": [
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "source": [
    "ITERATIVE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n          [-0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]]],\n\n\n        [[[ 0.1092,  0.0000,  0.0567,  ..., -0.0969,  0.0000,  0.1699],\n          [-0.1208,  0.0059,  0.0228,  ..., -0.1751, -0.0929, -0.0000],\n          [ 0.0000, -0.0000, -0.0398,  ..., -0.1616, -0.0858, -0.0612],\n          ...,\n          [ 0.1268,  0.0000,  0.0000,  ...,  0.0000, -0.0484,  0.0984],\n          [ 0.0000,  0.0135,  0.0963,  ...,  0.1790,  0.1162,  0.0350],\n          [ 0.0162,  0.1458,  0.1452,  ...,  0.0000,  0.0016,  0.0655]]],\n\n\n        [[[ 0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n          [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n          ...,\n          [ 0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]]],\n\n\n        ...,\n\n\n        [[[-0.1936, -0.1919, -0.0589,  ..., -0.0512, -0.0491, -0.1262],\n          [-0.1462,  0.0370, -0.0000,  ..., -0.0858, -0.0000, -0.1244],\n          [-0.0265,  0.0593, -0.0094,  ..., -0.1230,  0.0176, -0.0836],\n          ...,\n          [-0.0000, -0.0144, -0.0266,  ...,  0.0300, -0.0560,  0.0528],\n          [-0.0000,  0.0284, -0.1209,  ..., -0.1258, -0.1998, -0.0000],\n          [-0.0559,  0.0116, -0.1333,  ...,  0.0403, -0.0000, -0.2034]]],\n\n\n        [[[ 0.0774,  0.0112,  0.0000,  ...,  0.2061,  0.0000,  0.2375],\n          [ 0.0789, -0.0000,  0.1890,  ...,  0.0762, -0.0308,  0.1433],\n          [ 0.0701,  0.0000,  0.1911,  ...,  0.1647,  0.0801,  0.1504],\n          ...,\n          [ 0.0000, -0.0374, -0.0000,  ...,  0.0086, -0.0000,  0.1660],\n          [-0.0000,  0.1226, -0.0502,  ...,  0.0000,  0.0000,  0.0881],\n          [-0.0442,  0.0257,  0.0000,  ...,  0.0074,  0.0324,  0.0000]]],\n\n\n        [[[-0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n          [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n          [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n          ...,\n          [ 0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n          [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n          [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000]]]],\n       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "\n",
    "# As we can verify, this will zero out all the connections corresponding to\n",
    "# 50% (3 out of 6) of the channels, while preserving the action of the\n",
    "# previous mask.\n",
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[<torch.nn.utils.prune.RandomUnstructured object at 0x7fae529acd60>, <torch.nn.utils.prune.LnStructured object at 0x7fae517c9910>]\n"
     ]
    }
   ],
   "source": [
    "for hook in module._forward_pre_hooks.values():\n",
    "    if hook._tensor_name == \"weight\":  # select out the correct hook\n",
    "        break\n",
    "\n",
    "print(list(hook))  # pruning history in the container"
   ]
  },
  {
   "source": [
    "use the remove functionality from torch.nn.utils.prune. Note that this doesn’t undo the pruning, as if it never happened. It simply makes it permanent, instead, by reassigning the parameter weight to the model parameters, in its pruned version."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('weight', Parameter containing:\ntensor([[[[ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n          [-0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]]],\n\n\n        [[[ 0.1092,  0.0000,  0.0567,  ..., -0.0969,  0.0000,  0.1699],\n          [-0.1208,  0.0059,  0.0228,  ..., -0.1751, -0.0929, -0.0000],\n          [ 0.0000, -0.0000, -0.0398,  ..., -0.1616, -0.0858, -0.0612],\n          ...,\n          [ 0.1268,  0.0000,  0.0000,  ...,  0.0000, -0.0484,  0.0984],\n          [ 0.0000,  0.0135,  0.0963,  ...,  0.1790,  0.1162,  0.0350],\n          [ 0.0162,  0.1458,  0.1452,  ...,  0.0000,  0.0016,  0.0655]]],\n\n\n        [[[ 0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n          [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n          ...,\n          [ 0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]]],\n\n\n        ...,\n\n\n        [[[-0.1936, -0.1919, -0.0589,  ..., -0.0512, -0.0491, -0.1262],\n          [-0.1462,  0.0370, -0.0000,  ..., -0.0858, -0.0000, -0.1244],\n          [-0.0265,  0.0593, -0.0094,  ..., -0.1230,  0.0176, -0.0836],\n          ...,\n          [-0.0000, -0.0144, -0.0266,  ...,  0.0300, -0.0560,  0.0528],\n          [-0.0000,  0.0284, -0.1209,  ..., -0.1258, -0.1998, -0.0000],\n          [-0.0559,  0.0116, -0.1333,  ...,  0.0403, -0.0000, -0.2034]]],\n\n\n        [[[ 0.0774,  0.0112,  0.0000,  ...,  0.2061,  0.0000,  0.2375],\n          [ 0.0789, -0.0000,  0.1890,  ...,  0.0762, -0.0308,  0.1433],\n          [ 0.0701,  0.0000,  0.1911,  ...,  0.1647,  0.0801,  0.1504],\n          ...,\n          [ 0.0000, -0.0374, -0.0000,  ...,  0.0086, -0.0000,  0.1660],\n          [-0.0000,  0.1226, -0.0502,  ...,  0.0000,  0.0000,  0.0881],\n          [-0.0442,  0.0257,  0.0000,  ...,  0.0074,  0.0324,  0.0000]]],\n\n\n        [[[-0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n          [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n          [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n          ...,\n          [ 0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n          [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n          [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000]]]],\n       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "prune.remove(module, 'weight')\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "source": [
    "GLOBAL PRUNING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(num_classes=num_classes)\n",
    "model = load_model(model=model, model_filepath=model_filepath, device=cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.02315837775801753"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "evaluate_model(model=model, test_loader=val_loader, device=cpu_device, criterion=nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = (\n",
    "    (model.conv1, 'weight'),\n",
    "    (model.layer1[0].conv1, 'weight'),\n",
    "    (model.layer1[0].conv2, 'weight'),\n",
    "    (model.layer1[1].conv1, 'weight'),\n",
    "    (model.layer1[1].conv2, 'weight'),\n",
    "    (model.layer2[0].conv1, 'weight'),\n",
    "    (model.layer2[0].conv2, 'weight'),\n",
    "    (model.layer2[1].conv1, 'weight'),\n",
    "    (model.layer2[1].conv2, 'weight'),\n",
    "    (model.layer3[0].conv1, 'weight'),\n",
    "    (model.layer3[0].conv2, 'weight'),\n",
    "    (model.layer3[1].conv1, 'weight'),\n",
    "    (model.layer3[1].conv2, 'weight'),\n",
    "    (model.layer4[0].conv1, 'weight'),\n",
    "    (model.layer4[0].conv2, 'weight'),\n",
    "    (model.layer4[1].conv1, 'weight'),\n",
    "    (model.layer4[1].conv2, 'weight'),\n",
    "    (model.fc, 'weight'),\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(((Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       "   'weight'),\n",
       "  (Linear(in_features=512, out_features=136, bias=True), 'weight')),\n",
       " 18)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "parameters_to_prune, len(parameters_to_prune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "99, -0.3231,\n        -0.3468, -0.3553, -0.3537, -0.3315, -0.3713, -0.1091, -0.0959, -0.0258,\n        -0.2756, -0.2808, -0.2012, -0.2812, -0.1991, -0.3948, -0.2257, -0.2469,\n        -0.4211, -0.2110, -0.4670, -0.3069, -0.3549, -0.2337, -0.0612, -0.1321,\n        -0.2968, -0.1870, -0.2316, -0.0686, -0.3113, -0.2895, -0.3149, -0.2686,\n        -0.2081, -0.2096, -0.3011, -0.1810, -0.0227, -0.3873, -0.2665, -0.0225,\n        -0.2973, -0.0973, -0.2980, -0.3219, -0.2926, -0.3196, -0.4332, -0.1980,\n        -0.2117, -0.2302, -0.0980, -0.2344, -0.2154, -0.2921, -0.0350, -0.3361,\n        -0.2620, -0.2188, -0.1566, -0.1795, -0.2726, -0.4103,  0.0413, -0.1507,\n        -0.2552, -0.3137, -0.2466, -0.2961, -0.0938, -0.1481, -0.2129, -0.5480,\n        -0.2915, -0.2802, -0.5077, -0.1306, -0.1862, -0.2400, -0.4362, -0.3017,\n        -0.1633, -0.3447, -0.1047, -0.2846, -0.1244, -0.3036, -0.2404, -0.2333,\n        -0.2494, -0.1866, -0.3294, -0.1677, -0.2540, -0.1295, -0.0512, -0.1966,\n        -0.2801, -0.1702, -0.1879, -0.1850, -0.3274, -0.0369, -0.2979, -0.2612,\n        -0.1889, -0.3270, -0.1377, -0.2787, -0.2201, -0.2417, -0.2834, -0.0555,\n        -0.2538, -0.1040, -0.2660, -0.1644, -0.1723, -0.2672, -0.2797, -0.4214,\n        -0.0378, -0.2386, -0.3498, -0.2435, -0.4348, -0.2554, -0.1719, -0.2836,\n        -0.3316, -0.2787, -0.2879, -0.2640, -0.0560, -0.1789, -0.4195, -0.2152,\n         0.0567, -0.2359, -0.2249, -0.0911, -0.2644, -0.3875, -0.3317, -0.1415,\n        -0.3425, -0.0020, -0.1941, -0.2821, -0.2809, -0.0965, -0.1841, -0.2971,\n        -0.0173, -0.3043, -0.3013, -0.1729, -0.1872, -0.2683, -0.2033, -0.3059,\n        -0.2939, -0.2163, -0.1889, -0.2581, -0.2296, -0.2066, -0.3462, -0.4298,\n        -0.2600, -0.3095, -0.1800, -0.0116, -0.2124, -0.2552, -0.0523, -0.2216,\n        -0.2605, -0.2134, -0.2867, -0.2556, -0.2275, -0.3437, -0.1698, -0.1560,\n        -0.4120, -0.2067, -0.1159, -0.2408, -0.3093, -0.2621, -0.2593, -0.0135,\n        -0.3099, -0.2179, -0.2766, -0.2400, -0.3934,  0.0072, -0.2982, -0.0930,\n        -0.2166, -0.1635, -0.1827, -0.2308, -0.2525, -0.0991, -0.2325, -0.2938,\n        -0.2480, -0.0934, -0.1911, -0.3772, -0.3369, -0.1606, -0.2752, -0.3005,\n        -0.1372, -0.2990, -0.2156, -0.2622, -0.3160, -0.1342, -0.2903, -0.3865,\n        -0.2916, -0.3243, -0.2051, -0.2656, -0.2359, -0.1508, -0.1063, -0.3595,\n        -0.2312, -0.3046, -0.4178, -0.0276, -0.2204, -0.2426, -0.1616, -0.4789,\n        -0.1713, -0.2802, -0.2305, -0.4327, -0.2413, -0.1862, -0.1486, -0.1507])), ('layer4.1.conv2.weight_orig', Parameter containing:\ntensor([[[[ 2.8729e-04,  4.2632e-03, -2.0266e-03],\n          [ 1.9513e-04,  2.4381e-03, -5.8632e-03],\n          [ 4.4803e-03,  8.6577e-03,  8.5538e-04]],\n\n         [[-1.1335e-02, -1.3195e-02, -1.0305e-02],\n          [-4.9507e-03, -4.5898e-03, -3.1041e-03],\n          [-7.5883e-03, -8.3795e-03, -8.9239e-03]],\n\n         [[-1.1914e-02, -1.2104e-02, -1.0167e-02],\n          [-1.2093e-02, -1.1557e-02, -8.9600e-03],\n          [-1.2515e-02, -9.3296e-03, -6.4079e-03]],\n\n         ...,\n\n         [[-9.3573e-03, -1.0662e-02, -1.2672e-02],\n          [-8.0600e-03, -8.5423e-03, -1.2121e-02],\n          [-8.1498e-03, -8.8037e-03, -1.0611e-02]],\n\n         [[ 4.2632e-03,  5.6461e-03,  2.8460e-03],\n          [ 4.7070e-03,  6.2550e-03,  7.5862e-03],\n          [ 1.1504e-02,  1.1518e-02,  1.0728e-02]],\n\n         [[-6.2455e-03, -9.1693e-03, -9.6664e-03],\n          [-4.2935e-03, -6.5311e-03, -5.0513e-03],\n          [-3.1141e-03, -5.0124e-03, -5.8122e-03]]],\n\n\n        [[[ 2.7483e-03,  3.7146e-04,  3.3262e-05],\n          [-4.5675e-03, -6.6689e-03, -6.4447e-03],\n          [-6.7610e-03, -7.3204e-03, -9.5855e-03]],\n\n         [[-1.4630e-02, -1.2320e-02, -1.4457e-02],\n          [-8.6197e-03, -5.8059e-03, -1.1075e-02],\n          [-6.2154e-03, -6.8218e-03, -9.3805e-03]],\n\n         [[ 1.0879e-03,  4.3850e-04, -1.9456e-03],\n          [-1.2517e-03,  3.2917e-04, -2.1435e-03],\n          [ 4.8136e-03,  2.5333e-03,  5.1504e-03]],\n\n         ...,\n\n         [[ 2.4644e-02,  1.7434e-02,  2.0734e-02],\n          [ 2.3101e-02,  1.3487e-02,  2.0728e-02],\n          [ 1.9381e-02,  1.5243e-02,  1.7340e-02]],\n\n         [[ 1.2212e-02,  1.2448e-02,  1.5048e-02],\n          [ 5.2993e-03,  4.0090e-03,  9.3927e-03],\n          [ 6.6766e-03,  2.4941e-03,  8.3288e-03]],\n\n         [[ 3.1040e-02,  2.8243e-02,  3.2319e-02],\n          [ 3.8608e-02,  3.3099e-02,  3.8652e-02],\n          [ 2.5839e-02,  2.6524e-02,  2.4995e-02]]],\n\n\n        [[[-2.1761e-03,  4.5553e-03,  2.0612e-03],\n          [ 4.9747e-03,  1.1420e-02,  8.5734e-03],\n          [ 4.8583e-03,  1.1469e-02,  1.0039e-02]],\n\n         [[-6.2547e-05,  6.5336e-04,  9.4747e-04],\n          [ 5.0603e-03,  7.7136e-03,  6.5484e-03],\n          [-4.8432e-04,  2.3057e-03,  2.9219e-03]],\n\n         [[-3.2788e-02, -2.7615e-02, -3.2608e-02],\n          [-3.6296e-02, -2.8170e-02, -3.0277e-02],\n          [-3.6814e-02, -3.1547e-02, -3.0231e-02]],\n\n         ...,\n\n         [[-5.2998e-03, -2.8590e-04, -4.9266e-03],\n          [-7.0530e-03, -2.3684e-04, -1.5838e-03],\n          [-6.9291e-03,  4.8084e-04, -3.1548e-03]],\n\n         [[ 1.1854e-02,  8.4836e-03,  1.3839e-02],\n          [ 2.8741e-03, -9.7358e-05,  4.4888e-03],\n          [-2.5515e-03, -2.7788e-03, -3.2464e-03]],\n\n         [[-1.2408e-02, -1.5001e-02, -1.3377e-02],\n          [-1.4540e-02, -1.8537e-02, -1.7392e-02],\n          [-6.7315e-03, -9.5205e-03, -9.0692e-03]]],\n\n\n        ...,\n\n\n        [[[ 3.0369e-03,  1.9542e-03,  1.7140e-03],\n          [-7.6240e-03, -2.8765e-03, -5.1760e-03],\n          [-9.3019e-03, -4.8800e-03, -4.2932e-03]],\n\n         [[ 4.4836e-03,  2.4909e-03,  1.5746e-03],\n          [ 1.2065e-02,  1.2936e-02,  1.0344e-02],\n          [ 1.9010e-02,  1.7459e-02,  1.5988e-02]],\n\n         [[-1.4914e-03, -8.1727e-03, -8.0671e-03],\n          [-6.6247e-03, -6.2421e-03, -9.2717e-03],\n          [-8.7991e-03, -7.7528e-03, -8.6336e-03]],\n\n         ...,\n\n         [[-1.8040e-02, -1.5366e-02, -1.5334e-02],\n          [-1.3148e-02, -1.2180e-02, -1.0915e-02],\n          [-1.4545e-02, -1.4756e-02, -1.1787e-02]],\n\n         [[ 3.5762e-03,  6.6073e-03, -1.4055e-03],\n          [ 4.3975e-03,  7.8375e-03,  8.8085e-05],\n          [-5.0697e-03, -5.6633e-04, -5.9284e-03]],\n\n         [[-1.9234e-03, -8.8012e-03, -5.8821e-03],\n          [ 3.6685e-03, -1.3784e-03, -3.2117e-03],\n          [-4.7037e-04,  1.5340e-04, -3.4046e-03]]],\n\n\n        [[[-1.8305e-02, -1.7735e-02, -2.1683e-02],\n          [-1.6598e-02, -1.2508e-02, -2.0530e-02],\n          [-1.0800e-02, -9.8670e-03, -1.7195e-02]],\n\n         [[ 2.0721e-02,  2.2466e-02,  2.5049e-02],\n          [ 1.8682e-02,  1.3160e-02,  2.3696e-02],\n          [ 2.2104e-02,  1.7261e-02,  2.4877e-02]],\n\n         [[-5.7091e-03, -2.6876e-03, -9.2260e-04],\n          [-9.4530e-03, -7.0543e-03, -6.2770e-03],\n          [-4.5806e-03, -2.7182e-03, -2.5823e-03]],\n\n         ...,\n\n         [[ 2.4150e-02,  1.4002e-02,  1.6559e-02],\n          [ 2.1363e-02,  1.4359e-02,  1.5854e-02],\n          [ 2.5786e-02,  2.7233e-02,  2.5104e-02]],\n\n         [[-4.6450e-03,  1.2419e-03, -1.8768e-03],\n          [ 1.3005e-03,  4.0888e-03, -6.5483e-04],\n          [-7.9783e-03, -6.6539e-03, -8.9957e-03]],\n\n         [[ 1.1494e-02,  2.6621e-02,  1.5649e-02],\n          [ 6.5960e-03,  1.7290e-02,  7.5466e-03],\n          [-8.0256e-03,  4.6246e-03, -5.7808e-03]]],\n\n\n        [[[ 1.4232e-02,  1.1769e-02,  9.4342e-03],\n          [ 6.2592e-03,  5.1087e-03,  2.3311e-03],\n          [-1.9694e-03,  2.7110e-03, -2.8945e-03]],\n\n         [[-7.0772e-03,  1.0365e-03, -5.8451e-03],\n          [-9.1879e-03, -3.1388e-03, -8.1517e-03],\n          [-8.0300e-03, -5.1313e-03, -9.5734e-03]],\n\n         [[ 2.4314e-02,  1.8942e-02,  2.4256e-02],\n          [ 2.0090e-02,  1.1472e-02,  1.5993e-02],\n          [ 2.2910e-02,  2.0622e-02,  2.3820e-02]],\n\n         ...,\n\n         [[-1.6375e-02, -1.6928e-02, -1.9019e-02],\n          [-9.7367e-03, -1.1274e-02, -1.0261e-02],\n          [-1.2310e-02, -1.5931e-02, -1.4151e-02]],\n\n         [[ 4.7098e-03, -4.5205e-04,  2.8042e-03],\n          [ 2.1428e-03, -4.6175e-03, -1.6818e-03],\n          [-1.3336e-03, -5.5009e-03, -2.6237e-03]],\n\n         [[-1.4367e-02, -1.3520e-02, -1.1387e-02],\n          [-4.7420e-03, -1.7309e-03, -2.6426e-03],\n          [ 5.1448e-03,  7.0428e-03,  5.0202e-03]]]])), ('layer4.1.bn2.weight', Parameter containing:\ntensor([1.8419, 1.8307, 1.7650, 1.8288, 1.9505, 1.8026, 1.9536, 2.2790, 1.7662,\n        1.8902, 1.7768, 1.7749, 1.9055, 1.7328, 1.8762, 1.8211, 1.7967, 2.3428,\n        1.7985, 1.7271, 1.7915, 1.9512, 1.8928, 1.9017, 1.8784, 1.9809, 1.8569,\n        1.7830, 1.8911, 1.8859, 1.7764, 1.9832, 1.8389, 1.7616, 1.8728, 1.8753,\n        1.9008, 1.8209, 1.7039, 1.7377, 1.7786, 1.6944, 1.7829, 1.7815, 1.7594,\n        1.8428, 1.9238, 2.0871, 1.8980, 1.8413, 1.8471, 1.8584, 1.7640, 1.8453,\n        1.7606, 1.9504, 1.9620, 1.8755, 1.9424, 1.8731, 1.8674, 1.9422, 1.8750,\n        1.9208, 1.7464, 1.8558, 1.6539, 2.0660, 2.0298, 1.9174, 1.8972, 1.7589,\n        1.7551, 1.9560, 1.7909, 1.7971, 1.7851, 1.7733, 1.8061, 1.7949, 1.8169,\n        1.8089, 1.8641, 2.1542, 1.7739, 1.7913, 1.8022, 1.7155, 1.7679, 1.7704,\n        1.6266, 1.8645, 1.9076, 1.8576, 1.6924, 1.8020, 1.7100, 1.7713, 1.8572,\n        1.7103, 2.0664, 1.9054, 1.9422, 1.8078, 1.7412, 1.6061, 1.9105, 1.8947,\n        1.7954, 1.8989, 1.8239, 1.7619, 1.7951, 1.8149, 1.8539, 1.8502, 1.7095,\n        2.1831, 1.8599, 1.8252, 1.8193, 1.8460, 1.7968, 1.6229, 1.8450, 1.8290,\n        1.8706, 1.9293, 1.6881, 1.9725, 1.8981, 1.8925, 1.8851, 1.8445, 1.9764,\n        2.0674, 1.8384, 1.8414, 1.8762, 1.7931, 1.7131, 1.9644, 1.7854, 1.9369,\n        1.8972, 1.8940, 1.8700, 1.7967, 1.8775, 1.9409, 1.7391, 1.7944, 1.9678,\n        1.7678, 1.6851, 1.9414, 1.9663, 1.9882, 1.7915, 1.8141, 1.8325, 2.1200,\n        1.9256, 2.3592, 2.0304, 1.9594, 1.7334, 1.9048, 1.8221, 1.7811, 1.9084,\n        1.8053, 1.9171, 1.9644, 1.8256, 1.6432, 1.9173, 1.9094, 1.9923, 1.7963,\n        1.9077, 1.7619, 2.1724, 1.7931, 1.7564, 1.8889, 1.9832, 1.9136, 1.8035,\n        1.8419, 1.8278, 1.8057, 1.9063, 1.8646, 1.7848, 1.8230, 1.7986, 1.7091,\n        1.7724, 1.7939, 1.7611, 1.9325, 2.0162, 1.7295, 2.0196, 1.8876, 1.8325,\n        1.8225, 1.7870, 1.9160, 1.7197, 1.7170, 1.9133, 1.7770, 1.9943, 1.8389,\n        1.8070, 1.8516, 1.7857, 1.9648, 1.9553, 1.9232, 1.8086, 1.8114, 1.7141,\n        1.8058, 1.8532, 1.9255, 1.7682, 1.8314, 1.8495, 1.8296, 1.8278, 1.8819,\n        1.7698, 1.7838, 1.7807, 1.9974, 1.6994, 1.9483, 1.7793, 1.8029, 2.2210,\n        1.6455, 1.8357, 2.1706, 1.9204, 1.7414, 1.7809, 1.8648, 1.9145, 1.8849,\n        1.8346, 1.9368, 1.8169, 2.2302, 1.8262, 2.0651, 1.9888, 1.8169, 1.8462,\n        1.9681, 1.8083, 1.8595, 1.8539, 1.7699, 1.9001, 1.7285, 1.7553, 1.8924,\n        1.7829, 1.9428, 1.8724, 1.7228, 2.0548, 1.7732, 1.8561, 1.7699, 1.9269,\n        1.8171, 2.4075, 1.7257, 1.7819, 1.7244, 1.8521, 1.8302, 1.8797, 1.7617,\n        1.9650, 1.9807, 1.7102, 1.7486, 1.8350, 1.9919, 1.8505, 1.9000, 1.8269,\n        1.9787, 1.7635, 1.6071, 1.7998, 1.9545, 1.7348, 1.7140, 1.8851, 1.7981,\n        1.9100, 1.8315, 1.7864, 1.9165, 1.8839, 1.9017, 1.9334, 1.7405, 1.7661,\n        1.8015, 1.9987, 1.7622, 1.9107, 1.8444, 1.7128, 1.8726, 1.8529, 1.9270,\n        1.8769, 1.7261, 1.8393, 1.9075, 1.7953, 1.8246, 1.7605, 2.0470, 1.9221,\n        1.9205, 1.8910, 1.7666, 1.6801, 1.8308, 1.8845, 1.8339, 1.8238, 1.7616,\n        1.6114, 1.8411, 1.7437, 1.8423, 1.9540, 1.7465, 1.7741, 1.8746, 1.8856,\n        1.7740, 1.7603, 1.7682, 1.8396, 1.6869, 1.8080, 1.8836, 1.8283, 1.8341,\n        1.8522, 1.9749, 1.8707, 1.7719, 1.8993, 1.8108, 1.8480, 1.8267, 1.8731,\n        1.9576, 1.8347, 1.9509, 1.9641, 1.7997, 1.7652, 1.9253, 1.7126, 1.7551,\n        1.9427, 1.8559, 1.9163, 1.7681, 1.7803, 1.8500, 1.8535, 1.8865, 1.7599,\n        2.0692, 1.8021, 1.7077, 1.8890, 1.9457, 1.8516, 1.7882, 1.8356, 1.8472,\n        1.6708, 1.7435, 1.9080, 1.9653, 2.0401, 1.8935, 1.8450, 1.7536, 1.7733,\n        1.8135, 1.8534, 1.9368, 1.7348, 1.8738, 1.9632, 1.9033, 1.7422, 1.7842,\n        1.8516, 2.0218, 1.7044, 1.8793, 1.8655, 1.8516, 1.8002, 1.8687, 1.8460,\n        1.7589, 1.8174, 1.9830, 1.9034, 2.1222, 1.8460, 1.9209, 1.8893, 1.9422,\n        1.8489, 1.8396, 1.9953, 2.0865, 1.8253, 1.7700, 1.8035, 1.7535, 1.8923,\n        1.8620, 1.8627, 1.7264, 1.8140, 1.9613, 1.8812, 1.8729, 2.0050, 1.7092,\n        1.7726, 1.9410, 1.8381, 1.8366, 1.7276, 1.8796, 1.7548, 1.9536, 1.8062,\n        1.8883, 2.0278, 1.8775, 1.9446, 1.8676, 1.8423, 1.7798, 1.9403, 1.8375,\n        2.0473, 1.9507, 1.8337, 1.8184, 1.7791, 1.8993, 1.8781, 1.8691, 1.8493,\n        1.7623, 1.9458, 1.7564, 1.7448, 1.8633, 1.6863, 1.8062, 1.8702, 2.0048,\n        1.8504, 1.8964, 1.9489, 1.8264, 1.9019, 1.8196, 1.9712, 1.8969, 1.8652,\n        1.8709, 1.6984, 1.8677, 1.8846, 1.9256, 1.8620, 1.6366, 1.8434, 1.7506,\n        1.8438, 1.5788, 1.9316, 1.9535, 1.7878, 1.7354, 2.0920, 1.9456])), ('layer4.1.bn2.bias', Parameter containing:\ntensor([0.2371, 0.3433, 0.3279, 0.4642, 0.2233, 0.2370, 0.2176, 0.3793, 0.3140,\n        0.2803, 0.2434, 0.2116, 0.2478, 0.2435, 0.2298, 0.3172, 0.2725, 0.6511,\n        0.2925, 0.2281, 0.2279, 0.4254, 0.2342, 0.3328, 0.2632, 0.2176, 0.3180,\n        0.3893, 0.1387, 0.2274, 0.3379, 0.0767, 0.2253, 0.2504, 0.1990, 0.1951,\n        0.2566, 0.3253, 0.2797, 0.3149, 0.2373, 0.2533, 0.1956, 0.3236, 0.2093,\n        0.2333, 0.2300, 0.5019, 0.2830, 0.1885, 0.3264, 0.2722, 0.2369, 0.2430,\n        0.3625, 0.2165, 0.4700, 0.3047, 0.3675, 0.2641, 0.1979, 0.2664, 0.3448,\n        0.2005, 0.2450, 0.4351, 0.2689, 0.1632, 0.3087, 0.1209, 0.2153, 0.1592,\n        0.2960, 0.1423, 0.2951, 0.2706, 0.2007, 0.2939, 0.2210, 0.2243, 0.2465,\n        0.3910, 0.4599, 0.5417, 0.2147, 0.3469, 0.2703, 0.2229, 0.3645, 0.2647,\n        0.2421, 0.2492, 0.1666, 0.2763, 0.2560, 0.2151, 0.3363, 0.2767, 0.2516,\n        0.2988, 0.2622, 0.3499, 0.3001, 0.3907, 0.3184, 0.2233, 0.2649, 0.2110,\n        0.2034, 0.2752, 0.2314, 0.3480, 0.2238, 0.2892, 0.1991, 0.2923, 0.3259,\n        0.0722, 0.3039, 0.3041, 0.3803, 0.2568, 0.2382, 0.3057, 0.2652, 0.1532,\n        0.2110, 0.2567, 0.3148, 0.2746, 0.1833, 0.1950, 0.1116, 0.2279, 0.3705,\n        0.2477, 0.2000, 0.3060, 0.2548, 0.2468, 0.3028, 0.1921, 0.2952, 0.1980,\n        0.2135, 0.1583, 0.1586, 0.3944, 0.2352, 0.3947, 0.2740, 0.2861, 0.1856,\n        0.2702, 0.2986, 0.1728, 0.2658, 0.2696, 0.2028, 0.1838, 0.3176, 0.6246,\n        0.2631, 0.3855, 0.2074, 0.2317, 0.4171, 0.2044, 0.2926, 0.3506, 0.2305,\n        0.2400, 0.1420, 0.1093, 0.2757, 0.3253, 0.2334, 0.1650, 0.4026, 0.2066,\n        0.1790, 0.3032, 0.5658, 0.3246, 0.3834, 0.3254, 0.1772, 0.2909, 0.2350,\n        0.2519, 0.1968, 0.2003, 0.3213, 0.4802, 0.2543, 0.2578, 0.3280, 0.2270,\n        0.3044, 0.2273, 0.2447, 0.2527, 0.4136, 0.2588, 0.3589, 0.2688, 0.2115,\n        0.2022, 0.3186, 0.3740, 0.1785, 0.2074, 0.2346, 0.3566, 0.2623, 0.2620,\n        0.2880, 0.1462, 0.1896, 0.2777, 0.1852, 0.3240, 0.2748, 0.2164, 0.3066,\n        0.1845, 0.3992, 0.1695, 0.4411, 0.2812, 0.2730, 0.2784, 0.1861, 0.3589,\n        0.1934, 0.3320, 0.3350, 0.2655, 0.2740, 0.3185, 0.2633, 0.2458, 0.2003,\n        0.2809, 0.3049, 0.2050, 0.2904, 0.2381, 0.3278, 0.3484, 0.4293, 0.2422,\n        0.2859, 0.1864, 0.2954, 0.5634, 0.2081, 0.3743, 0.2902, 0.3820, 0.3069,\n        0.2101, 0.2750, 0.2878, 0.1870, 0.3015, 0.1661, 0.2998, 0.3101, 0.2522,\n        0.2419, 0.1758, 0.2681, 0.2812, 0.1495, 0.2868, 0.3157, 0.2587, 0.2437,\n        0.1467, 0.5416, 0.2490, 0.2831, 0.2783, 0.1614, 0.1963, 0.2034, 0.2364,\n        0.2527, 0.1573, 0.3184, 0.2841, 0.1613, 0.1489, 0.2850, 0.1625, 0.3277,\n        0.4936, 0.2780, 0.3178, 0.1743, 0.2158, 0.2222, 0.2821, 0.4267, 0.2713,\n        0.1778, 0.3067, 0.2270, 0.1772, 0.3897, 0.2923, 0.4843, 0.2345, 0.2327,\n        0.2740, 0.2700, 0.2804, 0.4035, 0.1501, 0.3329, 0.3286, 0.2803, 0.2309,\n        0.1738, 0.3270, 0.3097, 0.1808, 0.2384, 0.2107, 0.3240, 0.3346, 0.2236,\n        0.2061, 0.2687, 0.2360, 0.3338, 0.2694, 0.3203, 0.2895, 0.1884, 0.1491,\n        0.3957, 0.5167, 0.3407, 0.1854, 0.1816, 0.2626, 0.1855, 0.2219, 0.1482,\n        0.2584, 0.2458, 0.2616, 0.2396, 0.2402, 0.2423, 0.3463, 0.2731, 0.1524,\n        0.2514, 0.2760, 0.1734, 0.2715, 0.4052, 0.2252, 0.3676, 0.3070, 0.3127,\n        0.1836, 0.4330, 0.2203, 0.2073, 0.2803, 0.2984, 0.2191, 0.3272, 0.2267,\n        0.2749, 0.3056, 0.4566, 0.2962, 0.3528, 0.3236, 0.4220, 0.2715, 0.2256,\n        0.2903, 0.1829, 0.3994, 0.2820, 0.2471, 0.1647, 0.3654, 0.4504, 0.2685,\n        0.2992, 0.2825, 0.2435, 0.2212, 0.4300, 0.4342, 0.1988, 0.2863, 0.3398,\n        0.2444, 0.2905, 0.2559, 0.2586, 0.1702, 0.1906, 0.2536, 0.2978, 0.2498,\n        0.3777, 0.2252, 0.2472, 0.2243, 0.1732, 0.2194, 0.2091, 0.2820, 0.2898,\n        0.2887, 0.3292, 0.1644, 0.2962, 0.3279, 0.2535, 0.2795, 0.2238, 0.2607,\n        0.1937, 0.2680, 0.2418, 0.5193, 0.2502, 0.3147, 0.2166, 0.2313, 0.2027,\n        0.1880, 0.2180, 0.3826, 0.3871, 0.2358, 0.3556, 0.2272, 0.3272, 0.3442,\n        0.3154, 0.1993, 0.3135, 0.2254, 0.3048, 0.2658, 0.3337, 0.2679, 0.2670,\n        0.2363, 0.4347, 0.1931, 0.1995, 0.2072, 0.3202, 0.2667, 0.2305, 0.2383,\n        0.2246, 0.2562, 0.2837, 0.4046, 0.2786, 0.2243, 0.1591, 0.1923, 0.1894,\n        0.2496, 0.1140, 0.3128, 0.3197, 0.3530, 0.2999, 0.2115, 0.4718, 0.2979,\n        0.3472, 0.2890, 0.4740, 0.2230, 0.3630, 0.4015, 0.2446, 0.1897, 0.1460,\n        0.1874, 0.2734, 0.2366, 0.3001, 0.2359, 0.2688, 0.3256, 0.2749, 0.2848,\n        0.2299, 0.3001, 0.4818, 0.3074, 0.3164, 0.3114, 0.3549, 0.2859])), ('fc.bias', Parameter containing:\ntensor([ 1.7027e-02,  1.9902e-02,  1.7744e-02,  2.5932e-02, -2.0101e-02,\n        -1.5103e-02,  3.5616e-02, -2.2676e-02,  3.0125e-02,  4.8771e-04,\n        -2.5309e-02,  2.4356e-02, -1.6467e-03, -3.6222e-03, -3.3882e-02,\n        -1.8132e-02,  2.1925e-02, -2.9865e-02,  2.7111e-02,  4.6499e-02,\n        -2.7829e-03, -1.2196e-02,  4.5697e-02, -1.4532e-03, -5.0079e-03,\n         2.0707e-04,  3.8496e-02,  3.9079e-02,  1.3548e-02, -3.0523e-02,\n         2.5890e-02, -2.5210e-02,  1.3547e-02, -1.8585e-03,  1.6800e-02,\n        -2.0553e-02,  1.1079e-02,  5.7113e-03,  4.1257e-02,  1.6121e-02,\n        -2.1243e-02,  3.5659e-03, -4.7258e-03, -2.6501e-02,  2.8484e-02,\n         3.1527e-02, -1.1725e-02, -2.1279e-02,  1.4037e-02,  3.0765e-02,\n         4.1564e-03,  2.5834e-02,  3.7423e-02, -5.6251e-05,  3.8451e-03,\n        -1.9086e-02, -1.4384e-03,  2.5700e-02,  3.2940e-02,  8.6297e-03,\n         3.1022e-02, -3.6277e-02,  3.9932e-02, -3.7748e-02,  3.4670e-02,\n         8.6913e-03,  9.7703e-03,  3.3386e-02, -2.7306e-02, -3.9346e-03,\n        -3.7687e-02,  1.5214e-02, -3.1093e-02, -7.3357e-03,  2.9869e-02,\n         1.3441e-03, -2.7952e-02, -8.2450e-03,  2.6001e-02, -2.0850e-02,\n         1.0377e-03, -1.9164e-02,  3.6076e-03,  3.7176e-02,  1.4454e-02,\n         1.2699e-02, -3.1188e-02,  1.5076e-03,  3.4948e-02,  8.2785e-03,\n        -2.1129e-02, -2.4632e-02, -5.0234e-03, -3.4854e-02,  2.3426e-02,\n        -2.9055e-03,  3.7545e-02, -2.8443e-02,  2.5294e-03,  4.3382e-02,\n         2.2981e-02, -4.7563e-03,  3.8638e-02, -1.8252e-02,  1.9187e-02,\n        -3.9901e-02,  1.8405e-02,  1.7027e-02, -1.4811e-02, -3.6024e-02,\n         4.3780e-02,  4.1284e-03, -1.8821e-02,  4.3889e-02, -7.7219e-03,\n         3.7888e-02, -3.7467e-02,  1.7147e-02,  2.4403e-02, -2.0831e-02,\n         2.1788e-02,  1.1478e-02,  3.0164e-02,  4.3952e-02, -6.1361e-03,\n         9.0071e-03,  4.3430e-03, -2.9375e-02, -1.8122e-02, -1.8234e-02,\n        -2.2497e-02, -1.5662e-02,  1.0930e-02,  1.7003e-02,  4.4698e-02,\n         3.5102e-02], requires_grad=True)), ('fc.weight_orig', Parameter containing:\ntensor([[-0.0131, -0.0052, -0.0010,  ..., -0.0210, -0.0062,  0.0147],\n        [ 0.0395, -0.0028, -0.0037,  ...,  0.0390,  0.0192, -0.0170],\n        [-0.0020,  0.0136, -0.0329,  ...,  0.0154, -0.0042,  0.0209],\n        ...,\n        [-0.0136, -0.0016,  0.0170,  ..., -0.0100, -0.0222, -0.0310],\n        [-0.0231,  0.0217, -0.0029,  ..., -0.0043,  0.0164,  0.0254],\n        [ 0.0038,  0.0334,  0.0088,  ...,  0.0192,  0.0186,  0.0246]],\n       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sparsity in Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False): 2.46%\nSparsity in Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 21.34%\nSparsity in Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 10.19%\nSparsity in Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 9.85%\nSparsity in Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 10.06%\nSparsity in Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 11.12%\nSparsity in Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 12.89%\nSparsity in Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 12.74%\nSparsity in Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 13.23%\nSparsity in Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 14.39%\nSparsity in Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 15.85%\nSparsity in Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 17.14%\nSparsity in Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 18.36%\nSparsity in Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 18.17%\nSparsity in Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 20.23%\nSparsity in Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 19.02%\nSparsity in Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 26.77%\nSparsity in Linear(in_features=512, out_features=136, bias=True): 13.75%\nGlobal sparsity: 20.00%\n"
     ]
    }
   ],
   "source": [
    "sparsed = 0\n",
    "total = 0\n",
    "for param in parameters_to_prune:\n",
    "    sparsed += float(torch.sum(param[0].weight == 0))\n",
    "    total += float(param[0].weight.nelement())\n",
    "    prune.remove(param[0], \"weight\")\n",
    "    print(\"Sparsity in \" +str(param[0])+\": {:.2f}%\".format(\n",
    "        100. * float(torch.sum(param[0].weight == 0))\n",
    "        / float(param[0].weight.nelement())\n",
    "    ))\n",
    "    \n",
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * sparsed / total\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size (MB): 45.075501\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(model)"
   ]
  },
  {
   "source": [
    "no reduction in model size, only weights updated to 0 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.024139876099857124"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "evaluate_model(model=model, test_loader=val_loader, device=cpu_device, criterion=nn.MSELoss())"
   ]
  },
  {
   "source": [
    "EVAL LOSS NOT THAT HIGH"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Pruning with NNI - Neural Network Intelligence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(num_classes=num_classes)\n",
    "model = load_model(model=model, model_filepath=model_filepath, device=cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nni.algorithms.compression.pytorch.pruning import LevelPruner\n",
    "config_list = [{ 'sparsity': 0.2, 'op_types': ['default'] }]\n",
    "pruner = LevelPruner(model, config_list)\n",
    "pruned_model = pruner.compress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.02452341750734024"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "evaluate_model(model=pruned_model, test_loader=val_loader, device=cpu_device, criterion=nn.MSELoss())"
   ]
  },
  {
   "source": [
    "FOR REAL CHANGE IN THE MODEL AFTER PRUNING USE MODELSPEEDUP MODULE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nni.compression.pytorch import ModelSpeedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2021-01-22 09:58:39] INFO (nni.compression.pytorch.compressor/MainThread) Model state_dict saved to checkpoints_pruned/resnet18_FLM_pruned.pt\n",
      "[2021-01-22 09:58:39] INFO (nni.compression.pytorch.compressor/MainThread) Mask dict saved to checkpoints_pruned/mask.pth\n"
     ]
    }
   ],
   "source": [
    "prune_model_dir = \"checkpoints_pruned\"\n",
    "prune_model_filename = \"resnet18_FLM_pruned.pt\"\n",
    "prune_model_filepath = os.path.join(prune_model_dir, prune_model_filename)\n",
    "pruner.export_model(prune_model_filepath, os.path.join(prune_model_dir, 'mask.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = create_model(num_classes=num_classes)\n",
    "pruned_model.load_state_dict(torch.load(prune_model_filepath))\n",
    "masks_file = os.path.join(prune_model_dir, 'mask.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(32,1,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 224, 224])"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "dummy_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2021-01-22 10:01:57] INFO (nni.compression.pytorch.speedup.compressor/MainThread) start to speed up the model\n",
      "[2021-01-22 10:01:57] INFO (nni.compression.pytorch.speedup.compressor/MainThread) fix the mask conflict of the interdependent layers\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) {'conv1': 1, 'layer1.0.conv1': 1, 'layer1.0.conv2': 1, 'layer1.1.conv1': 1, 'layer1.1.conv2': 1, 'layer2.0.conv1': 1, 'layer2.0.conv2': 1, 'layer2.0.downsample.0': 1, 'layer2.1.conv1': 1, 'layer2.1.conv2': 1, 'layer3.0.conv1': 1, 'layer3.0.conv2': 1, 'layer3.0.downsample.0': 1, 'layer3.1.conv1': 1, 'layer3.1.conv2': 1, 'layer4.0.conv1': 1, 'layer4.0.conv2': 1, 'layer4.0.downsample.0': 1, 'layer4.1.conv1': 1, 'layer4.1.conv2': 1}\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers conv1 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer1.0.conv1 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer1.0.conv2 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer1.1.conv1 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer1.1.conv2 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer2.0.conv1 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer2.0.conv2 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer2.0.downsample.0 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer2.1.conv1 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer2.1.conv2 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer3.0.conv1 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer3.0.conv2 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer3.0.downsample.0 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer3.1.conv1 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer3.1.conv2 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer4.0.conv1 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer4.0.conv2 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer4.0.downsample.0 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer4.1.conv1 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) Layers layer4.1.conv2 using fine-grained pruning\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) dim0 sparsity: 0.000000\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) dim1 sparsity: 0.002083\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) detected conv prune dim: 1\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) fine-grained mask detected, skip solving conflict for this set: {'layer1.0.conv1', 'layer1.1.conv1', 'layer2.0.conv1', 'layer2.0.downsample.0'}\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) fine-grained mask detected, skip solving conflict for this set: {'layer2.1.conv1', 'layer3.0.downsample.0', 'layer3.0.conv1'}\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) fine-grained mask detected, skip solving conflict for this set: {'layer3.1.conv1', 'layer4.0.downsample.0', 'layer4.0.conv1'}\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.utils.mask_conflict/MainThread) fine-grained mask detected, skip solving conflict for this set: {'layer4.1.conv1', 'fc'}\n",
      "[2021-01-22 10:02:06] INFO (nni.compression.pytorch.speedup.compressor/MainThread) infer module masks...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (56) must match the size of tensor b (64) at non-singleton dimension 0",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-55-e74ca86a2468>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdevice\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cpu\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mm_speedup\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mModelSpeedup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpruned_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdummy_input\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmasks_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mm_speedup\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspeedup_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/nni/compression/pytorch/speedup/compressor.py\u001B[0m in \u001B[0;36mspeedup_model\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    181\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    182\u001B[0m         \u001B[0m_logger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"infer module masks...\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 183\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfer_modules_masks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    184\u001B[0m         \u001B[0m_logger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"replace compressed modules...\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    185\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreplace_compressed_modules\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/nni/compression/pytorch/speedup/compressor.py\u001B[0m in \u001B[0;36minfer_modules_masks\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    138\u001B[0m                 \u001B[0m_logger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwarning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'%s has mask, but not found in the traced graph, just skip it.'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodule_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m                 \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 140\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfer_module_mask\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    141\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    142\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mreplace_compressed_modules\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/nni/compression/pytorch/speedup/compressor.py\u001B[0m in \u001B[0;36minfer_module_mask\u001B[0;34m(self, module_name, last_module, mask, in_shape, out_shape)\u001B[0m\n\u001B[1;32m    118\u001B[0m             \u001B[0mpredecessors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtorch_graph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind_predecessors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0m_module_name\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpredecessors\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 120\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfer_module_mask\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_module_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_shape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minput_cmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    121\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0moutput_cmask\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m             \u001B[0msuccessors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtorch_graph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind_successors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/nni/compression/pytorch/speedup/compressor.py\u001B[0m in \u001B[0;36minfer_module_mask\u001B[0;34m(self, module_name, last_module, mask, in_shape, out_shape)\u001B[0m\n\u001B[1;32m    118\u001B[0m             \u001B[0mpredecessors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtorch_graph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind_predecessors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0m_module_name\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpredecessors\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 120\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfer_module_mask\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_module_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_shape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minput_cmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    121\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0moutput_cmask\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m             \u001B[0msuccessors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtorch_graph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind_successors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/nni/compression/pytorch/speedup/compressor.py\u001B[0m in \u001B[0;36minfer_module_mask\u001B[0;34m(self, module_name, last_module, mask, in_shape, out_shape)\u001B[0m\n\u001B[1;32m    118\u001B[0m             \u001B[0mpredecessors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtorch_graph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind_predecessors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0m_module_name\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpredecessors\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 120\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfer_module_mask\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_module_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_shape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minput_cmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    121\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0moutput_cmask\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m             \u001B[0msuccessors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtorch_graph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind_successors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/nni/compression/pytorch/speedup/compressor.py\u001B[0m in \u001B[0;36minfer_module_mask\u001B[0;34m(self, module_name, last_module, mask, in_shape, out_shape)\u001B[0m\n\u001B[1;32m    118\u001B[0m             \u001B[0mpredecessors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtorch_graph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind_predecessors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0m_module_name\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpredecessors\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 120\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfer_module_mask\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_module_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_shape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minput_cmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    121\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0moutput_cmask\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m             \u001B[0msuccessors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtorch_graph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind_successors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/nni/compression/pytorch/speedup/compressor.py\u001B[0m in \u001B[0;36minfer_module_mask\u001B[0;34m(self, module_name, last_module, mask, in_shape, out_shape)\u001B[0m\n\u001B[1;32m    113\u001B[0m                 \u001B[0minput_cmask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minfer_from_outshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mm_type\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_masks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_shape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtorch_graph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname_to_node\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mmodule_name\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauxiliary\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    114\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 115\u001B[0;31m                 \u001B[0minput_cmask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minfer_from_outshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mm_type\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_masks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    116\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    117\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0minput_cmask\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/nni/compression/pytorch/speedup/infer_shape.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(module_masks, mask)\u001B[0m\n\u001B[1;32m    293\u001B[0m     \u001B[0;34m'AdaptiveAvgPool2d'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mmodule_masks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mmaxpool2d_outshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_masks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    294\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 295\u001B[0;31m     \u001B[0;34m'ReLU'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mmodule_masks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mrelu_outshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_masks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    296\u001B[0m     \u001B[0;34m'ReLU6'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mmodule_masks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mrelu_outshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_masks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    297\u001B[0m     \u001B[0;34m'aten::relu'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mmodule_masks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mrelu_outshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule_masks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/nni/compression/pytorch/speedup/infer_shape.py\u001B[0m in \u001B[0;36mrelu_outshape\u001B[0;34m(module_masks, mask)\u001B[0m\n\u001B[1;32m    778\u001B[0m         \u001B[0;31m# mask conflict should be solved before speedup\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    779\u001B[0m         assert all(\n\u001B[0;32m--> 780\u001B[0;31m             module_masks.output_mask.mask_index[1] == mask.mask_index[1])\n\u001B[0m\u001B[1;32m    781\u001B[0m     \u001B[0mmodule_masks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_input_mask\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m     \u001B[0mmodule_masks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_output_mask\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mwrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     25\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mhandle_torch_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwrapped\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mNotImplemented\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (56) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "m_speedup = ModelSpeedup(pruned_model, dummy_input, masks_file, device)\n",
    "m_speedup.speedup_model()"
   ]
  },
  {
   "source": [
    "EXISTING ISSUE. WATCH V2.0"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "PRUNING WITH TENSORFLOW (TFMOT) LEADS TO REAL MODEL COMPRESSION, BUT REQUIRES THE MODEL DEFINITION TO BE IN TF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "ADD EXAMPLE WITH TENSORFLOW HERE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}