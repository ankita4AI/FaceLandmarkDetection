{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from FaceLandmarkDetection.src.detection.model.resnet import resnet18\n",
    "from FaceLandmarkDetection.src.detection.data.dataset import FaceLandmarksDataset\n",
    "from FaceLandmarkDetection.src.detection.data.prepare_data import Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FaceLandmarkDetection.config import data_dir, data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'FaceLandMarkDetection/data/face_landmark_dataset'\n",
    "data_file = 'FaceLandMarkDetection/data/face_landmark_dataset/labels_ibug_300W_train.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = FaceLandmarksDataset(data_file=data_file, data_dir=data_dir, transform=Transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6666"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transformed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds(random_seed=0):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    \n",
    "def get_data():\n",
    "    transformed_dataset = FaceLandmarksDataset(data_file=data_file, data_dir=data_dir, transform=Transforms())\n",
    "    toy_factor = 1/6\n",
    "    toy_dataset, other_dataset = torch.utils.data.random_split(transformed_dataset,\n",
    "                                              [int(len(transformed_dataset)*toy_factor), int(len(transformed_dataset)*(1-toy_factor))])\n",
    "    # split the dataset into validation and test sets\n",
    "    print(len(toy_dataset))\n",
    "    len_valid_set = int(0.1 * len(toy_dataset))\n",
    "    len_train_set = len(toy_dataset) - len_valid_set\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(toy_dataset, [len_train_set, len_valid_set])\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "\n",
    "def make_loader(dataset, batch_size):\n",
    "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=2)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device, criterion=None):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.view(labels.size(0), -1).float()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        if criterion is not None:\n",
    "            loss = criterion(outputs, labels).item()\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss * inputs.size(0)\n",
    "        \n",
    "    eval_loss = running_loss / len(test_loader.dataset)\n",
    "    return eval_loss\n",
    "\n",
    "def train_model(model, train_loader, test_loader, device, num_epochs):\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        # pruner.update_epoch(epoch)\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.view(labels.size(0), -1).float()\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
    "        print(\"Epoch: {:02d} Train Loss: {:.3f} Eval Loss: {:.3f}\".format(epoch, train_loss, eval_loss))\n",
    "\n",
    "    return model\n",
    "\n",
    "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.view(labels.size(0), -1).float()\n",
    "        _ = model(inputs)\n",
    "\n",
    "def measure_inference_latency(model, device, input_size=(1,1,224,224), num_samples=100):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_samples):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        _ = model(x)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_time_ave = elapsed_time / num_samples\n",
    "    return elapsed_time_ave\n",
    "\n",
    "def save_model(model, model_dir, model_filename):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.save(model.state_dict(), model_filepath)\n",
    "\n",
    "def load_model(model, model_filepath, device):\n",
    "    model.load_state_dict(torch.load(model_filepath, map_location=lambda storage, loc: storage))\n",
    "    return model\n",
    "\n",
    "def save_torchscript_model(model, model_dir, model_filename):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
    "\n",
    "def load_torchscript_model(model_filepath, device):\n",
    "    model = torch.jit.load(model_filepath, map_location=device)\n",
    "    return model\n",
    "\n",
    "def create_model(num_classes=10):\n",
    "    model = resnet18(pretrained=True)\n",
    "\n",
    "    # We would use the pretrained ResNet18 as a feature extractor.\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Modify the input channels\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    \n",
    "    # Modify the last FC layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "num_classes = 136\n",
    "cuda_device = torch.device(\"cuda:0\")\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "model_dir = \"saved_models\"\n",
    "model_filename = \"resnet18_FLM.pt\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "set_random_seeds(random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111\n"
     ]
    }
   ],
   "source": [
    "# Create an untrained model.\n",
    "model = create_model(num_classes=num_classes)\n",
    "train_dataset, val_dataset = get_data()\n",
    "train_loader = make_loader(train_dataset, 64)\n",
    "val_loader = make_loader(val_dataset, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 Train Loss: 3.717 Eval Loss: 1.787\n",
      "Epoch: 01 Train Loss: 0.651 Eval Loss: 0.451\n",
      "Epoch: 02 Train Loss: 0.196 Eval Loss: 0.141\n",
      "Epoch: 03 Train Loss: 0.083 Eval Loss: 0.064\n",
      "Epoch: 04 Train Loss: 0.052 Eval Loss: 0.048\n",
      "Epoch: 05 Train Loss: 0.041 Eval Loss: 0.039\n",
      "Epoch: 06 Train Loss: 0.034 Eval Loss: 0.035\n",
      "Epoch: 07 Train Loss: 0.029 Eval Loss: 0.030\n",
      "Epoch: 08 Train Loss: 0.026 Eval Loss: 0.027\n",
      "Epoch: 09 Train Loss: 0.023 Eval Loss: 0.024\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model=model, train_loader=train_loader, test_loader=val_loader, device=cpu_device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.\n",
    "save_model(model=model, model_dir=model_dir, model_filename=model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}